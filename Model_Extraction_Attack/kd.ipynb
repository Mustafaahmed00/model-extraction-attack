{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:29:36.165626Z",
     "start_time": "2025-04-17T20:29:36.162269Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# IMPORTS\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras import metrics, losses\n",
    "from keras.models import load_model\n",
    "import urllib.request\n",
    "import os\n",
    "import tarfile\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:29:37.264455Z",
     "start_time": "2025-04-17T20:29:37.036626Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load both the teacher and student model\n",
    "try:\n",
    "    scratch_student = load_model('student_model.h5')\n",
    "    student_model = load_model('student_model.h5')\n",
    "    teacher_model = load_model('teacher_model.h5')\n",
    "    \n",
    "    # Compile models with appropriate settings\n",
    "    scratch_student.compile(optimizer='adam',\n",
    "                          loss='categorical_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "    \n",
    "    student_model.compile(optimizer='adam',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         metrics=['accuracy'])\n",
    "    \n",
    "    teacher_model.compile(optimizer='adam',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         metrics=['accuracy'])\n",
    "    \n",
    "    print(\"Models loaded and compiled successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading models: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:29:43.040437Z",
     "start_time": "2025-04-17T20:29:41.574357Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load STL-10 dataset for knowledge distillation\n",
    "# This dataset will be used to train the student model\n",
    "# Note: This is a similar but different dataset from what was used to train the teacher model\n",
    "\n",
    "# Load training images\n",
    "print(\"Loading STL-10 training images...\")\n",
    "train_images = []\n",
    "train_path = 'STL-10/train_images'\n",
    "for img_path in glob.glob(os.path.join(train_path, '*.*')):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_array = np.array(img)\n",
    "    train_images.append(img_array)\n",
    "X_train = np.array(train_images)\n",
    "\n",
    "# Load test images\n",
    "print(\"Loading STL-10 test images...\")\n",
    "test_images = []\n",
    "test_path = 'STL-10/test_images'\n",
    "for img_path in glob.glob(os.path.join(test_path, '*.*')):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_array = np.array(img)\n",
    "    test_images.append(img_array)\n",
    "X_test = np.array(test_images)\n",
    "\n",
    "# Normalize the data (same as CIFAR-10)\n",
    "X_train = X_train.astype('float32')/255.0\n",
    "X_test = X_test.astype('float32')/255.0\n",
    "\n",
    "# Create labels (0-9 for 10 classes)\n",
    "y_train = np.zeros(len(X_train))  # We'll update these with actual labels\n",
    "y_test = np.zeros(len(X_test))    # We'll update these with actual labels\n",
    "\n",
    "# Convert labels to one-hot encoding (same as CIFAR-10)\n",
    "test_labels = to_categorical(y_test)\n",
    "train_labels = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:29:45.341429Z",
     "start_time": "2025-04-17T20:29:45.334879Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute student model metrics without KD\n",
    "scratch_student.compile(optimizer = 'adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:31:52.282960Z",
     "start_time": "2025-04-17T20:29:47.700252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.2982 - loss: 2.1188\n",
      "Epoch 2/7\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.4839 - loss: 1.4265\n",
      "Epoch 3/7\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.5555 - loss: 1.2539\n",
      "Epoch 4/7\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.5952 - loss: 1.1463\n",
      "Epoch 5/7\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.6246 - loss: 1.0649\n",
      "Epoch 6/7\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.6404 - loss: 1.0242\n",
      "Epoch 7/7\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.6625 - loss: 0.9699\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e12c245130>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let us try to see what if we directly train the student model without using knowledge distillation\n",
    "scratch_student.fit(X_train, train_labels, epochs=7, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:31:59.629278Z",
     "start_time": "2025-04-17T20:31:58.175683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5412 - loss: 1.3541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.356394648551941, 0.5432999730110168]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We evaluate student model for its loss and accuracy, if the student model is trained without using knowledge distillation\n",
    "scratch_student.evaluate(X_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:32:04.949533Z",
     "start_time": "2025-04-17T20:32:04.944367Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now let us try using knowledge distillation\n",
    "# KNOWLEDGE DISTILLATION CLASS, You can adjust alpha based on how much you want the student to learn from the teacher\n",
    "\n",
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.2,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        \"\"\"Configure the distiller.\n",
    "\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \"\"\"\n",
    "        super().compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def compute_loss(\n",
    "        self, x=None, y=None, y_pred=None, sample_weight=None, allow_empty=False\n",
    "    ):\n",
    "        teacher_pred = self.teacher(x, training=False)\n",
    "        student_loss = self.student_loss_fn(y, y_pred)\n",
    "\n",
    "        distillation_loss = self.distillation_loss_fn(\n",
    "            tf.nn.softmax(teacher_pred / self.temperature, axis=1),\n",
    "            tf.nn.softmax(y_pred / self.temperature, axis=1),\n",
    "        ) * (self.temperature**2)\n",
    "\n",
    "        loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "        return loss\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.student(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:34:23.933303Z",
     "start_time": "2025-04-17T20:32:07.940862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - categorical_accuracy: 0.3646 - loss: 2.1855 - val_categorical_accuracy: 0.5998 - val_loss: 2.0767\n",
      "Epoch 2/7\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - categorical_accuracy: 0.6438 - loss: 2.0573 - val_categorical_accuracy: 0.6967 - val_loss: 2.0333\n",
      "Epoch 3/7\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - categorical_accuracy: 0.7336 - loss: 2.0163 - val_categorical_accuracy: 0.7351 - val_loss: 2.0208\n",
      "Epoch 4/7\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - categorical_accuracy: 0.7750 - loss: 1.9952 - val_categorical_accuracy: 0.7445 - val_loss: 2.0144\n",
      "Epoch 5/7\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - categorical_accuracy: 0.8180 - loss: 1.9741 - val_categorical_accuracy: 0.7457 - val_loss: 2.0213\n",
      "Epoch 6/7\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - categorical_accuracy: 0.8467 - loss: 1.9598 - val_categorical_accuracy: 0.7484 - val_loss: 2.0247\n",
      "Epoch 7/7\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - categorical_accuracy: 0.8819 - loss: 1.9437 - val_categorical_accuracy: 0.7485 - val_loss: 2.0328\n"
     ]
    }
   ],
   "source": [
    "# Initialize the distiller\n",
    "# Train the student model using knowledge distillation\n",
    "distiller = Distiller(student=student_model, teacher=teacher_model)\n",
    "\n",
    "# Compiling the Distiller. You can adjust alpha based on how much you want the student to learn from the teacher\n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[metrics.CategoricalAccuracy()],\n",
    "    student_loss_fn=losses.CategoricalCrossentropy(),\n",
    "    distillation_loss_fn=losses.CategoricalCrossentropy(),\n",
    "    alpha=0.2,\n",
    "    temperature=1,\n",
    ") \n",
    "\n",
    "# Fitting the student model receiving KD\n",
    "history = distiller.fit(\n",
    "    X_train,\n",
    "    train_labels,\n",
    "    epochs=7,\n",
    "    batch_size=32,  \n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:34:33.939276Z",
     "start_time": "2025-04-17T20:34:32.153616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - categorical_accuracy: 0.7383 - loss: 2.0353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.03804087638855, 0.7369999885559082]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We evaluate student model again for its loss and accuracy,\n",
    "# But this time the student model is trained using knowledge distillation\n",
    "# You can compare this results with the results above\n",
    "distiller.evaluate(X_test, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
