{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:29:36.165626Z",
     "start_time": "2025-04-17T20:29:36.162269Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# IMPORTS\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras import metrics, losses\n",
    "from keras.models import load_model\n",
    "import urllib.request\n",
    "import os\n",
    "import tarfile\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:29:37.264455Z",
     "start_time": "2025-04-17T20:29:37.036626Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded and compiled successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load both teacher and student models\n",
    "try:\n",
    "    teacher_model = load_model('teacher_model.h5')\n",
    "    scratch_student = load_model('student_model.h5')\n",
    "    \n",
    "    # Compile teacher model\n",
    "    teacher_model.compile(optimizer='adam',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         metrics=['accuracy'])\n",
    "    \n",
    "    print(\"Models loaded and compiled successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading models: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:29:43.040437Z",
     "start_time": "2025-04-17T20:29:41.574357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading STL-10 training images...\n",
      "Loading STL-10 test images...\n"
     ]
    }
   ],
   "source": [
    "# Load STL-10 dataset for model extraction attack\n",
    "print(\"Loading STL-10 training images...\")\n",
    "train_images = []\n",
    "train_path = 'STL-10/train_images'\n",
    "for img_path in glob.glob(os.path.join(train_path, '*.*')):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img = img.resize((32, 32))  # Resize to 32x32\n",
    "    img_array = np.array(img)\n",
    "    train_images.append(img_array)\n",
    "X_train = np.array(train_images)\n",
    "\n",
    "# Load test images\n",
    "print(\"Loading STL-10 test images...\")\n",
    "test_images = []\n",
    "test_path = 'STL-10/test_images'\n",
    "for img_path in glob.glob(os.path.join(test_path, '*.*')):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img = img.resize((32, 32))  # Resize to 32x32\n",
    "    img_array = np.array(img)\n",
    "    test_images.append(img_array)\n",
    "X_test = np.array(test_images)\n",
    "\n",
    "# Normalize the data\n",
    "X_train = X_train.astype('float32')/255.0\n",
    "X_test = X_test.astype('float32')/255.0\n",
    "\n",
    "# Create labels (0-9 for 10 classes)\n",
    "y_train = np.zeros(len(X_train))  # We'll update these with teacher's predictions\n",
    "y_test = np.zeros(len(X_test))    # We'll update these with teacher's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:29:45.341429Z",
     "start_time": "2025-04-17T20:29:45.334879Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute student model metrics without KD\n",
    "# Compute student model metrics without KD (baseline performance)\n",
    "scratch_student.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', 'precision', 'recall']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:31:52.282960Z",
     "start_time": "2025-04-17T20:29:47.700252Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate training labels from teacher model predictions\n",
    "print(\"Generating training labels from teacher model...\")\n",
    "train_labels = teacher_model.predict(X_train)\n",
    "test_labels = teacher_model.predict(X_test)\n",
    "\n",
    "# First, let us try to see what if we directly train the student model without using knowledge distillation\n",
    "scratch_student.fit(X_train, train_labels, epochs=7, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:31:59.629278Z",
     "start_time": "2025-04-17T20:31:58.175683Z"
    }
   },
   "outputs": [],
   "source": [
    "# We evaluate student model for its loss and accuracy, if the student model is trained without using knowledge distillation\n",
    "print(\"\\nEvaluating student model performance without knowledge distillation:\")\n",
    "test_loss, test_accuracy = scratch_student.evaluate(X_test, test_labels)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Get predictions for confusion matrix\n",
    "y_pred = scratch_student.predict(X_test)\n",
    "y_true = np.argmax(test_labels, axis=1)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = tf.math.confusion_matrix(y_true, y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Student Model (Without KD)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:32:04.949533Z",
     "start_time": "2025-04-17T20:32:04.944367Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now let us try using knowledge distillation\n",
    "# KNOWLEDGE DISTILLATION CLASS, You can adjust alpha based on how much you want the student to learn from the teacher\n",
    "\n",
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.2,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        \"\"\"Configure the distiller.\n",
    "\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \"\"\"\n",
    "        super().compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def compute_loss(\n",
    "        self, x=None, y=None, y_pred=None, sample_weight=None, allow_empty=False\n",
    "    ):\n",
    "        teacher_pred = self.teacher(x, training=False)\n",
    "        student_loss = self.student_loss_fn(y, y_pred)\n",
    "\n",
    "        distillation_loss = self.distillation_loss_fn(\n",
    "            tf.nn.softmax(teacher_pred / self.temperature, axis=1),\n",
    "            tf.nn.softmax(y_pred / self.temperature, axis=1),\n",
    "        ) * (self.temperature**2)\n",
    "\n",
    "        loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "        return loss\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.student(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:34:23.933303Z",
     "start_time": "2025-04-17T20:32:07.940862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing knowledge distillation training...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'student_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Initialize the distiller\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Train the student model using knowledge distillation\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mInitializing knowledge distillation training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m distiller = Distiller(student=\u001b[43mstudent_model\u001b[49m, teacher=teacher_model)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Compiling the Distiller with enhanced metrics\u001b[39;00m\n\u001b[32m      7\u001b[39m distiller.compile(\n\u001b[32m      8\u001b[39m     optimizer=keras.optimizers.Adam(learning_rate=\u001b[32m0.001\u001b[39m),\n\u001b[32m      9\u001b[39m     metrics=[\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     temperature=\u001b[32m1\u001b[39m,  \u001b[38;5;66;03m# Temperature for softening probabilities\u001b[39;00m\n\u001b[32m     18\u001b[39m ) \n",
      "\u001b[31mNameError\u001b[39m: name 'student_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize the distiller\n",
    "# Train the student model using knowledge distillation\n",
    "print(\"\\nInitializing knowledge distillation training...\")\n",
    "distiller = Distiller(student=scratch_student, teacher=teacher_model)  # Use scratch_student instead of student_model\n",
    "\n",
    "# Compiling the Distiller with enhanced metrics\n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=[\n",
    "        metrics.CategoricalAccuracy(),\n",
    "        metrics.Precision(),\n",
    "        metrics.Recall()\n",
    "    ],\n",
    "    student_loss_fn=losses.CategoricalCrossentropy(),\n",
    "    distillation_loss_fn=losses.CategoricalCrossentropy(),\n",
    "    alpha=0.2,  # Weight for student loss\n",
    "    temperature=1,  # Temperature for softening probabilities\n",
    ") \n",
    "\n",
    "# Fitting the student model with knowledge distillation\n",
    "print(\"\\nTraining student model with knowledge distillation...\")\n",
    "history = distiller.fit(\n",
    "    X_train,\n",
    "    train_labels,\n",
    "    epochs=7,\n",
    "    batch_size=32,  \n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=2,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['categorical_accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_categorical_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:34:33.939276Z",
     "start_time": "2025-04-17T20:34:32.153616Z"
    }
   },
   "outputs": [],
   "source": [
    "# We evaluate student model again for its loss and accuracy,\n",
    "# But this time the student model is trained using knowledge distillation\n",
    "print(\"\\nEvaluating student model performance with knowledge distillation:\")\n",
    "test_loss, test_accuracy, test_precision, test_recall = distiller.evaluate(X_test, test_labels)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "# Get predictions for confusion matrix\n",
    "y_pred = distiller.predict(X_test)\n",
    "y_true = np.argmax(test_labels, axis=1)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = tf.math.confusion_matrix(y_true, y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Student Model (With Knowledge Distillation)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Compare with baseline performance\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(\"Without Knowledge Distillation:\")\n",
    "print(f\"Accuracy: {0.5433:.4f}\")  # Your baseline accuracy\n",
    "print(\"\\nWith Knowledge Distillation:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Improvement: {(test_accuracy - 0.5433)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
